{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368dec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64201fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b459faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b76427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import pickle\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import torch\n",
    "from torchvision import models as tm\n",
    "import shutil\n",
    "import util\n",
    "import utils\n",
    "import torchaudio\n",
    "import models\n",
    "import dataloaders\n",
    "from stats import calculate_stats, d_prime\n",
    "sys.path.append('/VocalSound-project/src')\n",
    "import ast\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import json\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9547f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: data}, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# for train, validation, test\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m json_files \u001b[38;5;241m=\u001b[39m get_immediate_files(\u001b[43mdata_dir\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/datafiles/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json_f \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_immediate_files(a_dir):\n",
    "    return [name for name in os.listdir(a_dir) if os.path.isfile(os.path.join(a_dir, name))]\n",
    "\n",
    "def change_path(json_file_path, target_path):\n",
    "    with open(json_file_path, 'r') as fp:\n",
    "        data_json = json.load(fp)\n",
    "    data = data_json['data']\n",
    "\n",
    "    # change the path in the json file\n",
    "    for i in range(len(data)):\n",
    "        ori_path = data[i][\"wav\"]\n",
    "        new_path = target_path + '/data/audio_16k/' + ori_path.split('/')[-1]\n",
    "        data[i][\"wav\"] = new_path\n",
    "\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump({'data': data}, f, indent=1)\n",
    "\n",
    "# for train, validation, test\n",
    "json_files = get_immediate_files(data_dir + '/data/datafiles/')\n",
    "for json_f in json_files:\n",
    "    if json_f.endswith('.json'):\n",
    "        print('now processing ' + data_dir + '/data/datafiles/' + json_f)\n",
    "        change_path(data_dir + '/data/datafiles/' + json_f, data_dir)\n",
    "\n",
    "# for subtest sets\n",
    "json_files = get_immediate_files(data_dir + '/data/datafiles/subtest/')\n",
    "for json_f in json_files:\n",
    "    if json_f.endswith('.json'):\n",
    "        print('now processing ' + data_dir + '/data/datafiles/subtest/' + json_f)\n",
    "        change_path(data_dir + '/data/datafiles/subtest/' + json_f, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f72852",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m audio_model \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_model' is not defined"
     ]
    }
   ],
   "source": [
    "del audio_model \n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5de09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am process 250732, running on udc-ba27-26: starting (Sun Aug  7 14:17:28 2022)\n"
     ]
    }
   ],
   "source": [
    "print(\"I am process %s, running on %s: starting (%s)\" % (\n",
    "        os.getpid(), os.uname()[1], time.asctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3001d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e076bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNetOri(torch.nn.Module):\n",
    "    def __init__(self, label_dim=6, pretrain=True, level=0):\n",
    "        super().__init__()\n",
    "        b = int(level)\n",
    "        if pretrain == True:\n",
    "            print('now train a effnet-b{:d} model with ImageNet pretrain'.format(b))\n",
    "        else:\n",
    "            print('now train a effnet-b{:d} model without ImageNet pretrain'.format(b))\n",
    "        if b == 7:\n",
    "            override_params = {'width_coefficient':0.8,\n",
    "                'depth_coefficient':0.7,\n",
    "                'image_size':128,\n",
    "                'dropout_rate':None,\n",
    "                'num_classes':6,\n",
    "                'batch_norm_momentum':.84,\n",
    "                'batch_norm_epsilon':5e-3,\n",
    "                'drop_connect_rate':None,\n",
    "                'depth_divisor':8,\n",
    "                'min_depth':None,\n",
    "                'include_top':None}           \n",
    "            globalp = utils.get_model_params('efficientnet-b7',override_params=override_params)\n",
    "            self._global_params = globalp[1]\n",
    "            self._blocks_args = globalp[0]\n",
    "            self.model = tm.efficientnet_b7(self._blocks_args,self._global_params, pretrained=pretrain) \n",
    "        elif b == 6:\n",
    "            self.model = tm.efficientnet_b6(pretrained=pretrain)\n",
    "        elif b == 5:\n",
    "            self.model = tm.efficientnet_b5(pretrained=pretrain)\n",
    "        elif b == 4:\n",
    "            self.model = tm.efficientnet_b4(pretrained=pretrain)\n",
    "        elif b == 3:\n",
    "            self.model = tm.efficientnet_b3(pretrained=pretrain)\n",
    "        elif b == 2:\n",
    "            self.model = tm.efficientnet_b2(pretrained=pretrain)\n",
    "        elif b == 1:\n",
    "            self.model = tm.efficientnet_b1(pretrained=pretrain)\n",
    "        elif b == 0:\n",
    "            self.model = tm.efficientnet_b0(pretrained=pretrain)\n",
    "\n",
    "        new_proj = torch.nn.Conv2d(3, 32, kernel_size=(3,3), stride=(2,2), padding=(1,1), bias=False)\n",
    "        print('conv1 get from pretrained model.')\n",
    "        new_proj.weight = torch.nn.Parameter(torch.sum(self.model.features[0][0].weight, dim=1).unsqueeze(1))\n",
    "        new_proj.bias = self.model.features[0][0].bias\n",
    "        self.model.features[0][0] = new_proj\n",
    "        self.model = create_feature_extractor(self.model, {'features.8': 'mout'})\n",
    "        self.feat_dim, self.freq_dim = self.get_dim()\n",
    "\n",
    "    def get_dim(self):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = torch.zeros(10, 1000, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.model(x)['mout']\n",
    "        return int(x.shape[1]), int(x.shape[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.model(x)['mout']\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        #x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "951a3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MSDSNet(nn.Module):    \n",
    "    def __init__(self, global_params=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 3, 3, 1)\n",
    "        self.conv2_bn = nn.BatchNorm1d(510)\n",
    "        #self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        #self.conv4 = nn.Conv2d(128, 3, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((8, 8)) # extended\n",
    "        self.fc1 = nn.Linear(64, 10)\n",
    "        self.fc2 = nn.Linear(10, 6)\n",
    "        #self.fc3 = nn.Linear(10, 6)\n",
    "        #assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        #assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Args:\n",
    "            inputs (tensor): Input tensor.\n",
    "        Returns:\n",
    "            Output of this model after processing.\n",
    "        \"\"\"\n",
    "        # Pooling and final linear layer\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        #x = self.conv3(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.conv4(x)\n",
    "        #x = F.relu(x)\n",
    "\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,-1)\n",
    "\n",
    "    def get_image_size(cls, model_name):\n",
    "        \"\"\"Get the input image size for a given efficientnet model.\n",
    "        Args:\n",
    "            model_name (str): Name for efficientnet.\n",
    "        Returns:\n",
    "            Input image size (resolution).\n",
    "        \"\"\"\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    def _change_in_channels(self, in_channels):\n",
    "        \"\"\"Adjust model's first convolution layer to in_channels, if in_channels not equals 3.\n",
    "        Args:\n",
    "            in_channels (int): Input data's channel number.\n",
    "        \"\"\"\n",
    "        if in_channels != 3:\n",
    "            Conv2d = get_same_padding_conv2d(image_size = self._global_params.image_size)\n",
    "            out_channels = round_filters(32, self._global_params)\n",
    "            self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0f2c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=os.getcwd() #change to your directory\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--data-train\", type=str, default=f'{data_dir}/data/datafiles/tr.json')\n",
    "parser.add_argument(\"--data-val\", type=str, default=f'{data_dir}/data/datafiles/val.json')\n",
    "parser.add_argument(\"--label-csv\", type=str, default=f'{data_dir}/class_labels_indices_vs.csv')\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=f'/exp')\n",
    "# training and optimization args\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "parser.add_argument('-b', '--batch-size', default=3, type=int, metavar='N', help='mini-batch size (default: 100)')\n",
    "parser.add_argument('-w', '--num-workers', default=4, type=int, metavar='NW', help='# of workers for dataloading (default: 8)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--lr-decay', default=None, type=int, metavar='LRDECAY', help='Divide the learning rate by 10 every lr_decay epochs')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-7, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=20, help=\"number of maximum training epochs\")\n",
    "parser.add_argument(\"--n-print-steps\", type=int, default=100, help=\"number of steps to print statistics\")\n",
    "# models args\n",
    "parser.add_argument(\"--n_class\", type=int, default=6, help=\"number of classes\")\n",
    "parser.add_argument('--save_model', help='save the models or not', type=ast.literal_eval, default='True')\n",
    "parser.add_argument(\"--model\", type=str, default='msds', help=\"model\")\n",
    "parser.add_argument(\"--model_size\", type=int, default=8, help=\"model size\")\n",
    "parser.add_argument('--imagenet_pretrain', help='if use pretrained imagenet efficient net', type=ast.literal_eval, default='True')\n",
    "parser.add_argument('--freqm', help='frequency mask max length', type=int, default=48)\n",
    "parser.add_argument('--timem', help='time mask max length', type=int, default=192)\n",
    "parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "390a63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'msds'\n",
    "if model == 'msds':\n",
    "    audio_model = MSDSNet()\n",
    "elif model == 'EffNet':\n",
    "    audio_model = EffNetOri(label_dim=6, level=7)\n",
    "else:\n",
    "    raise ValueError('Model Unrecognized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6fbc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def train(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.set_grad_enabled(True)\n",
    "    best_epoch, best_cum_epoch, best_mAP, best_acc, best_cum_mAP = 0, 0, -np.inf, -np.inf, -np.inf\n",
    "    global_step, epoch = 0, 0\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    audio_trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1000000))\n",
    "    print('Total trainable parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_trainables) / 1000000))\n",
    "    trainables = audio_trainables\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=args.weight_decay, betas=(0.95, 0.999))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(10, 60)), gamma=1.0)\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "    \n",
    "    result = np.zeros([args.n_epochs, 9])\n",
    "    audio_model.train()\n",
    "    while epoch < args.n_epochs + 1:\n",
    "        audio_model.train()\n",
    "\n",
    "        for i, (audio_input, labels) in enumerate(train_loader):\n",
    "            # measure data loading time\n",
    "            B = audio_input.size(0)\n",
    "            audio_input = audio_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "            audio_output = audio_model(audio_input)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "\n",
    "            # original optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            end_time = time.time()\n",
    "            #print(end_time)\n",
    "            global_step += 1\n",
    "        print(loss)\n",
    "        print('start validation')\n",
    "        stats, valid_loss = validate(audio_model, train_loader, args, epoch)\n",
    "\n",
    "        cum_stats = stats\n",
    "\n",
    "        cum_mAP = np.mean([stat['AP'] for stat in cum_stats])\n",
    "        cum_mAUC = np.mean([stat['auc'] for stat in cum_stats])\n",
    "        cum_acc = np.mean([stat['acc'] for stat in cum_stats])\n",
    "\n",
    "        mAP = np.mean([stat['AP'] for stat in stats])\n",
    "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "        acc = np.mean([stat['acc'] for stat in stats])\n",
    "\n",
    "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "        average_precision = np.mean(middle_ps)\n",
    "        average_recall = np.mean(middle_rs)\n",
    "\n",
    "        print(\"---------------------Epoch {:d} Results---------------------\".format(epoch))\n",
    "        #print(\"ACC: {:.6f}\".format(acc))\n",
    "        print(\"ACC:\", acc)\n",
    "        print(\"mAP: {:.6f}\".format(mAP))\n",
    "        print(\"AUC: {:.6f}\".format(mAUC))\n",
    "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "\n",
    "        result[epoch-1, :] = [mAP, acc, average_precision, average_recall, d_prime(mAUC), valid_loss, cum_mAP, cum_acc, optimizer.param_groups[0]['lr']]\n",
    "\n",
    "        #np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
    "\n",
    "        #if acc > best_acc:\n",
    "            #best_acc = acc\n",
    "            #best_acc_epoch = epoch\n",
    "            #torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        #print('number of params groups:' + str(len(optimizer.param_groups)))\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "def validate(audio_model, val_loader, args, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    audio_model = audio_model\n",
    "    audio_model.eval()\n",
    "\n",
    "    A_predictions, A_targets, A_loss = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_input, labels) in enumerate(val_loader):\n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            # compute output\n",
    "            audio_output = audio_model(audio_input)\n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "            A_predictions.append(predictions)\n",
    "            A_targets.append(labels)\n",
    "\n",
    "            # compute the loss\n",
    "            labels = labels.to(device)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "        audio_output = torch.cat(A_predictions)\n",
    "        target = torch.cat(A_targets)\n",
    "        loss = np.mean(A_loss)\n",
    "        stats = calculate_stats(audio_output, target)\n",
    "\n",
    "    return stats, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16a6e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = {'num_mel_bins': 128, 'target_length': 512, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'mode': 'train'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70548611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced sampler is not used\n"
     ]
    }
   ],
   "source": [
    "print('balanced sampler is not used')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataloaders.VSDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, raw_wav_mode=False, specaug=True),\n",
    "    batch_size=args.batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8f31f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': 512, 'mixup': 0, 'mode': 'test'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a459c88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataloaders.VSDataset(args.data_val, label_csv=args.label_csv, audio_conf=val_audio_conf, raw_wav_mode=False),\n",
    "    batch_size=args.batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93f4ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating experiment directory: /exp\n",
      "Now starting training for 20 epochs\n",
      "Total parameter number is : 0.003 million\n",
      "Total trainable parameter number is : 0.003 million\n",
      "current #steps=0, #epochs=1\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 1 Results---------------------\n",
      "ACC: 0.16953190393406736\n",
      "mAP: 0.167956\n",
      "AUC: 0.501039\n",
      "Avg Precision: 0.166766\n",
      "Avg Recall: 0.499035\n",
      "d_prime: 0.003684\n",
      "valid_loss: 1.798882\n",
      "Epoch-1 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 2 Results---------------------\n",
      "ACC: 0.16727834653274096\n",
      "mAP: 0.168800\n",
      "AUC: 0.505626\n",
      "Avg Precision: 0.168166\n",
      "Avg Recall: 0.504874\n",
      "d_prime: 0.019944\n",
      "valid_loss: 1.794609\n",
      "Epoch-2 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 3 Results---------------------\n",
      "ACC: 0.16927435451677295\n",
      "mAP: 0.168612\n",
      "AUC: 0.504609\n",
      "Avg Precision: 0.170266\n",
      "Avg Recall: 0.508349\n",
      "d_prime: 0.016338\n",
      "valid_loss: 1.792837\n",
      "Epoch-3 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 4 Results---------------------\n",
      "ACC: 0.16972506599703818\n",
      "mAP: 0.172978\n",
      "AUC: 0.513339\n",
      "Avg Precision: 0.174568\n",
      "Avg Recall: 0.399256\n",
      "d_prime: 0.047295\n",
      "valid_loss: 1.791509\n",
      "Epoch-4 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 5 Results---------------------\n",
      "ACC: 0.16528233854870902\n",
      "mAP: 0.172762\n",
      "AUC: 0.512123\n",
      "Avg Precision: 0.172448\n",
      "Avg Recall: 0.475810\n",
      "d_prime: 0.042980\n",
      "valid_loss: 1.791857\n",
      "Epoch-5 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 6 Results---------------------\n",
      "ACC: 0.17024016483162707\n",
      "mAP: 0.171977\n",
      "AUC: 0.508151\n",
      "Avg Precision: 0.169987\n",
      "Avg Recall: 0.487736\n",
      "d_prime: 0.028896\n",
      "valid_loss: 1.791661\n",
      "Epoch-6 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 7 Results---------------------\n",
      "ACC: 0.18974953319168117\n",
      "mAP: 0.178186\n",
      "AUC: 0.521715\n",
      "Avg Precision: 0.175468\n",
      "Avg Recall: 0.518254\n",
      "d_prime: 0.077015\n",
      "valid_loss: 1.790670\n",
      "Epoch-7 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 8 Results---------------------\n",
      "ACC: 0.20307771553666856\n",
      "mAP: 0.189679\n",
      "AUC: 0.545704\n",
      "Avg Precision: 0.187294\n",
      "Avg Recall: 0.561332\n",
      "d_prime: 0.162372\n",
      "valid_loss: 1.786646\n",
      "Epoch-8 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 9 Results---------------------\n",
      "ACC: 0.19792672719077972\n",
      "mAP: 0.189787\n",
      "AUC: 0.540768\n",
      "Avg Precision: 0.182392\n",
      "Avg Recall: 0.544721\n",
      "d_prime: 0.144772\n",
      "valid_loss: 1.786772\n",
      "Epoch-9 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 10 Results---------------------\n",
      "ACC: 0.21795119438542268\n",
      "mAP: 0.200514\n",
      "AUC: 0.564806\n",
      "Avg Precision: 0.194822\n",
      "Avg Recall: 0.584320\n",
      "d_prime: 0.230751\n",
      "valid_loss: 1.781041\n",
      "Epoch-10 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 11 Results---------------------\n",
      "ACC: 0.2344987444465907\n",
      "mAP: 0.222979\n",
      "AUC: 0.590200\n",
      "Avg Precision: 0.202592\n",
      "Avg Recall: 0.607758\n",
      "d_prime: 0.322526\n",
      "valid_loss: 1.771168\n",
      "Epoch-11 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 12 Results---------------------\n",
      "ACC: 0.24177451548515871\n",
      "mAP: 0.230585\n",
      "AUC: 0.601214\n",
      "Avg Precision: 0.206849\n",
      "Avg Recall: 0.620701\n",
      "d_prime: 0.362732\n",
      "valid_loss: 1.768730\n",
      "Epoch-12 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 13 Results---------------------\n",
      "ACC: 0.25819329083767956\n",
      "mAP: 0.236206\n",
      "AUC: 0.614967\n",
      "Avg Precision: 0.213302\n",
      "Avg Recall: 0.639954\n",
      "d_prime: 0.413359\n",
      "valid_loss: 1.761932\n",
      "Epoch-13 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 14 Results---------------------\n",
      "ACC: 0.25046680831884616\n",
      "mAP: 0.235097\n",
      "AUC: 0.613844\n",
      "Avg Precision: 0.213605\n",
      "Avg Recall: 0.641051\n",
      "d_prime: 0.409207\n",
      "valid_loss: 1.753894\n",
      "Epoch-14 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 15 Results---------------------\n",
      "ACC: 0.2625072435773614\n",
      "mAP: 0.245684\n",
      "AUC: 0.622612\n",
      "Avg Precision: 0.215907\n",
      "Avg Recall: 0.647293\n",
      "d_prime: 0.441726\n",
      "valid_loss: 1.751577\n",
      "Epoch-15 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 16 Results---------------------\n",
      "ACC: 0.27853969480394053\n",
      "mAP: 0.258771\n",
      "AUC: 0.641513\n",
      "Avg Precision: 0.222954\n",
      "Avg Recall: 0.668929\n",
      "d_prime: 0.512662\n",
      "valid_loss: 1.738757\n",
      "Epoch-16 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 17 Results---------------------\n",
      "ACC: 0.27853969480394053\n",
      "mAP: 0.256457\n",
      "AUC: 0.641989\n",
      "Avg Precision: 0.224068\n",
      "Avg Recall: 0.672600\n",
      "d_prime: 0.514465\n",
      "valid_loss: 1.735318\n",
      "Epoch-17 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 18 Results---------------------\n",
      "ACC: 0.28143712574850305\n",
      "mAP: 0.266204\n",
      "AUC: 0.649827\n",
      "Avg Precision: 0.225454\n",
      "Avg Recall: 0.676462\n",
      "d_prime: 0.544266\n",
      "valid_loss: 1.731000\n",
      "Epoch-18 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 19 Results---------------------\n",
      "ACC: 0.28298242225226966\n",
      "mAP: 0.267753\n",
      "AUC: 0.647258\n",
      "Avg Precision: 0.223240\n",
      "Avg Recall: 0.669766\n",
      "d_prime: 0.534469\n",
      "valid_loss: 1.727279\n",
      "Epoch-19 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "start validation\n",
      "---------------------Epoch 20 Results---------------------\n",
      "ACC: 0.29302684952675295\n",
      "mAP: 0.274777\n",
      "AUC: 0.657847\n",
      "Avg Precision: 0.229024\n",
      "Avg Recall: 0.687088\n",
      "d_prime: 0.575011\n",
      "valid_loss: 1.714585\n",
      "Epoch-20 lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------evaluate on the validation set---------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VSDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, cur_eval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_eval_list):\n\u001b[1;32m     36\u001b[0m     cur_eval \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m cur_eval\n\u001b[1;32m     37\u001b[0m     eval_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m---> 38\u001b[0m         \u001b[43mVSDataset\u001b[49m(cur_eval, label_csv\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlabel_csv, audio_conf\u001b[38;5;241m=\u001b[39mval_audio_conf),\n\u001b[1;32m     39\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m     stats, _ \u001b[38;5;241m=\u001b[39m validate(audio_model, eval_loader, args, eval_name_list[idx])\n\u001b[1;32m     41\u001b[0m     eval_acc \u001b[38;5;241m=\u001b[39m stats[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VSDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "if os.path.exists(args.exp_dir):\n",
    "    print(\"Deleting existing experiment directory %s\" % args.exp_dir)\n",
    "    shutil.rmtree(args.exp_dir)\n",
    "print(\"\\nCreating experiment directory: %s\" % args.exp_dir)\n",
    "#os.makedirs(\"%s/models\" % args.exp_dir)\n",
    "#with open(\"%s/args.pkl\" % args.exp_dir, \"wb\") as f:\n",
    "#    pickle.dump(args, f)\n",
    "\n",
    "print('Now starting training for {:d} epochs'.format(args.n_epochs))\n",
    "train(audio_model, train_loader, val_loader, args)\n",
    "\n",
    "# test on the test set and sub-test set, model selected on the validation set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#sd = torch.load(args.exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "#audio_model.load_state_dict(sd)\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# best model on the validation set, repeat to confirm\n",
    "stats, _ = validate(audio_model, train_loader, args, 'valid_set')\n",
    "\n",
    "# note it is NOT mean of class-wise accuracy\n",
    "val_acc = stats\n",
    "#val_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "print('---------------evaluate on the validation set---------------')\n",
    "#print(\"Accuracy: {:.6f}\".format(val_acc))\n",
    "all_res.append(val_acc)\n",
    "\n",
    "# test the model on the evaluation set\n",
    "data_eval_list = ['te.json', 'subtest/te_age1.json', 'subtest/te_age2.json', 'subtest/te_age3.json', 'subtest/te_female.json', 'subtest/te_male.json']\n",
    "eval_name_list = ['all_test', 'test age 18-25', 'test age 26-48', 'test age 49-80', 'test female', 'test male']\n",
    "\n",
    "data_dir = '/'.join(args.data_val.split('/')[:-1])\n",
    "for idx, cur_eval in enumerate(data_eval_list):\n",
    "    cur_eval = data_dir + '/' + cur_eval\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        VSDataset(cur_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, args, eval_name_list[idx])\n",
    "    eval_acc = stats[0]['acc']\n",
    "    all_res.append(eval_acc)\n",
    "    print('---------------evaluate on {:s}---------------'.format(eval_name_list[idx]))\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "\n",
    "all_res = np.array(all_res)\n",
    "all_res = all_res.reshape([1, all_res.shape[0]])\n",
    "#np.savetxt(args.exp_dir + '/all_eval_result.csv', all_res, header=','.join(['validation'] + eval_name_list), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9de06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the test set and sub-test set, model selected on the validation set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sd = torch.load(exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "audio_model = torch.nn.DataParallel(audio_model)\n",
    "audio_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58599369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------evaluate on all_test---------------\n",
      "Accuracy: 0.338346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250732/4026164799.py:51: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 512, 128] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m cur_eval \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m cur_eval\n\u001b[1;32m      8\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      9\u001b[0m     dataloaders\u001b[38;5;241m.\u001b[39mVSDataset(cur_eval, label_csv\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlabel_csv, audio_conf\u001b[38;5;241m=\u001b[39mval_audio_conf),\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m stats, _ \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_name_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m eval_acc \u001b[38;5;241m=\u001b[39m stats[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m all_res\u001b[38;5;241m.\u001b[39mappend(eval_acc)\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(audio_model, val_loader, args, epoch)\u001b[0m\n\u001b[1;32m    105\u001b[0m audio_input \u001b[38;5;241m=\u001b[39m audio_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m audio_output \u001b[38;5;241m=\u001b[39m \u001b[43maudio_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m predictions \u001b[38;5;241m=\u001b[39m audio_output\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    111\u001b[0m A_predictions\u001b[38;5;241m.\u001b[39mappend(predictions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mMSDSNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"Args:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    inputs (tensor): Input tensor.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Output of this model after processing.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Pooling and final linear layer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 512, 128] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# test the model on the evaluation set\n",
    "data_eval_list = ['te.json', 'subtest/te_age1.json', 'subtest/te_age2.json', 'subtest/te_age3.json', 'subtest/te_female.json', 'subtest/te_male.json']\n",
    "eval_name_list = ['all_test', 'test age 18-25', 'test age 26-48', 'test age 49-80', 'test female', 'test male']\n",
    "\n",
    "data_dir = '/'.join(args.data_val.split('/')[:-1])\n",
    "for idx, cur_eval in enumerate(data_eval_list):\n",
    "    cur_eval = data_dir + '/' + cur_eval\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataloaders.VSDataset(cur_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, args, eval_name_list[idx])\n",
    "    eval_acc = stats[0]['acc']\n",
    "    all_res.append(eval_acc)\n",
    "    print('---------------evaluate on {:s}---------------'.format(eval_name_list[idx]))\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "\n",
    "all_res = np.array(all_res)\n",
    "all_res = all_res.reshape([1, all_res.shape[0]])\n",
    "#np.savetxt(args.exp_dir + '/all_eval_result.csv', all_res, header=','.join(['validation'] + eval_name_list), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada28f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on the evaluation set\n",
    "data_eval_list = ['te.json', 'subtest/te_age1.json', 'subtest/te_age2.json', 'subtest/te_age3.json', 'subtest/te_female.json', 'subtest/te_male.json']\n",
    "eval_name_list = ['all_test', 'test age 18-25', 'test age 26-48', 'test age 49-80', 'test female', 'test male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9cb56e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, cur_eval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_eval_list):\n\u001b[1;32m      3\u001b[0m     cur_eval \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m cur_eval\n\u001b[1;32m      4\u001b[0m     eval_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      5\u001b[0m         dataloaders\u001b[38;5;241m.\u001b[39mVSDataset(cur_eval, label_csv\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlabel_csv, audio_conf\u001b[38;5;241m=\u001b[39maudio_conf),\n\u001b[0;32m----> 6\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     stats, _ \u001b[38;5;241m=\u001b[39m validate(audio_model, eval_loader, exp_dir, eval_name_list[idx])\n\u001b[1;32m      8\u001b[0m     eval_acc \u001b[38;5;241m=\u001b[39m stats[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "#data_dir = '/'.join(data_val.split('/')[:-1])\n",
    "for idx, cur_eval in enumerate(data_eval_list):\n",
    "    cur_eval = data_dir + '/' + cur_eval\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataloaders.VSDataset(cur_eval, label_csv=args.label_csv, audio_conf=audio_conf),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, exp_dir, eval_name_list[idx])\n",
    "    eval_acc = stats[0]['acc']\n",
    "    all_res.append(eval_acc)\n",
    "    print('---------------evaluate on {:s}---------------'.format(eval_name_list[idx]))\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = np.array(all_res)\n",
    "all_res = all_res.reshape([1, all_res.shape[0]])\n",
    "np.savetxt(exp_dir + '/all_eval_result.csv', all_res, header=','.join(['validation'] + eval_name_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
