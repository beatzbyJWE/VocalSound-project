{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5577acb3-8fb0-4d87-8e5b-9b6e8bb76e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39cf181-a5dd-48a6-89bb-ef1367b192b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df8da8f-40bd-4d75-804a-b7baea1c0fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 01:42:51.852958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:51.858296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:51.858612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:51.859715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-04 01:42:51.860140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:51.860374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:51.860585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:52.138313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:52.138535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:52.138713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-04 01:42:52.138866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024*8)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fefebb-f30b-48d6-81ff-e2275c8650a9",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f2a6b4-7ebe-4be9-8283-f69dab6f14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = '/home/jake/project/split'\n",
    "data_dir_path = pathlib.Path(data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3753d-c182-4dc2-8d57-c6db4b8ef519",
   "metadata": {},
   "source": [
    "## New Spectro Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9446f7-786a-4ada-85a3-d995462fb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050 # sample rate\n",
    "file_format = \"wav\" # or wav, ...\n",
    "num_channels = 1 # mono\n",
    "n_fft = 2048 # number of fft length. 2**n\n",
    "win_length = 1000 # window length <= n_fft\n",
    "hop_length = 250 # hopping step\n",
    "n_mels = 80 # number of mels\n",
    "n_mfccs = 40 # number of mfccs\n",
    "preemp = .97 # preemphasis rate\n",
    "n_iter = 50 # Griffin-Lim's law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce03015-bc34-4450-915e-29972f2f2d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338173a3-4be2-4a1d-80d0-622f9b98968b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c4e1e10-1c9b-4962-853e-abb6fb5920d7",
   "metadata": {},
   "source": [
    "## Original Spectro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5eabbf74-8fdc-44eb-ba1b-9ba94fefd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "  input_len = 16000\n",
    "  waveform = waveform[:input_len]\n",
    "  zero_padding = tf.zeros(\n",
    "      [16000] - tf.shape(waveform),\n",
    "      dtype=tf.float32)\n",
    "  # Cast the waveform tensors' dtype to float32.\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "  # clips are of the same length.\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd30e28-70ac-44f3-89aa-eec22ec69e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "  # Since all the data is single channel (mono), drop the `channels`\n",
    "  # axis from the array.\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f271a53-3294-462d-a29a-5d20ea2fa6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # Convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  one_hot = parts[-2] == class_names\n",
    "  # Integer encode the label\n",
    "  return tf.argmax(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17b5f59f-3418-42d5-8846-e2d832d3ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4bbf9d3-05fb-4b5b-a2e4-d0cc9b363edb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [16000,1] vs. shape[1] = [0,15999] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_audio\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mget_spectrogram\u001b[0;34m(waveform)\u001b[0m\n\u001b[1;32m      9\u001b[0m waveform \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(waveform, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Concatenate the waveform with `zero_padding`, which ensures all audio\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# clips are of the same length.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m equal_length \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_padding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert the waveform to a spectrogram via a STFT.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mstft(\n\u001b[1;32m     15\u001b[0m     equal_length, frame_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, frame_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [16000,1] vs. shape[1] = [0,15999] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "get_spectrogram(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81050804-635a-474a-a30c-656a24a15346",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608f66d2-e961-483f-b912-cb47b63bca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/home/jake/project/split/val/sigh/f1961_0_sigh.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fa1794-a7ca-4655-8cde-8d9d88b51cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64171, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = tf.io.read_file('/home/jake/project/split/val/sigh/f1961_0_sigh.wav')\n",
    "test_audio, _ = tf.audio.decode_wav(contents=test_file)\n",
    "test_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84c1b4c-8d76-4acf-aacd-3c1760aa85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = tf.squeeze(test_audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ee83f7-061e-40cd-a296-890a27f24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "def preemphasize(y, rate=.97):\n",
    "    '''\n",
    "    y: 1-D tensor. Waveform.\n",
    "    rate: A python scalar.\n",
    "    '''\n",
    "    y = tf.concat((y[:1], y[1:]-rate*y[:-1]), -1)\n",
    "    return y\n",
    "\n",
    "test_audio = preemphasize(test_audio, preemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65dd3bc9-b7a7-41db-b955-e6726a6e079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74530/3382435735.py:7: FutureWarning: Pass sr=22050, n_fft=2048, n_mels=80 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = tf.convert_to_tensor(librosa.filters.mel(sr, n_fft, n_mels), tf.float32)\n",
      "2022-08-04 01:43:23.837034: E tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n",
      "2022-08-04 01:43:23.837049: E tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2022-08-04 01:43:23.837063: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         mel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear, mag, mel\n\u001b[0;32m---> 13\u001b[0m linear, mag, mel \u001b[38;5;241m=\u001b[39m \u001b[43mget_spectrograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(linear\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mshape, mag\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mshape, mel\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mget_spectrograms\u001b[0;34m(y, sr, n_fft, win_length, hop_length, n_mels, power)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_mels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     mel_basis \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(librosa\u001b[38;5;241m.\u001b[39mfilters\u001b[38;5;241m.\u001b[39mmel(sr, n_fft, n_mels), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 8\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmag\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (t, n_mels)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInternalError\u001b[0m: Attempting to perform BLAS operation using StreamExecutor without BLAS support [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# TF\n",
    "def get_spectrograms(y, sr=22050, n_fft=2048, win_length=2048, hop_length=512, n_mels=None, power=1):\n",
    "    linear = tf.signal.stft(y, frame_length=win_length, frame_step=hop_length, fft_length=n_fft) # linear spectrogram\n",
    "    mag = tf.abs(linear) # magnitude\n",
    "    \n",
    "    if n_mels is not None:\n",
    "        mel_basis = tf.convert_to_tensor(librosa.filters.mel(sr, n_fft, n_mels), tf.float32)\n",
    "        mel = tf.matmul(mag**power, mel_basis, transpose_b=True) # (t, n_mels)\n",
    "    else:\n",
    "        mel = None\n",
    "    \n",
    "    return linear, mag, mel\n",
    "linear, mag, mel = get_spectrograms(test_audio, sr, n_fft, win_length, hop_length, n_mels)\n",
    "print(linear.eval().shape, mag.eval().shape, mel.eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c6131-081a-4991-b39b-7e7d790d3a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca0d0-0a21-4bf4-bd69-36475b66c357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96992f1-5c7d-48ad-b76d-241498438984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "753e8a88-e486-45c8-b94d-9b6968021edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69bf41ad-835a-435e-bc3d-59ca80a75f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jake/project/split')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa23c59c-cbe9-48a8-baca-a7c77d60deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dc55907-7e3c-4df3-87b9-1a5cd45af441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files('/home/jake/project/split/train/*/*', shuffle=True)\n",
    "test_ds = tf.data.Dataset.list_files('/home/jake/project/split/test/*/*', shuffle=True)\n",
    "val_ds = tf.data.Dataset.list_files('/home/jake/project/split/val/*/*', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6d085d2-fcc5-42d8-8b21-02b83cb39bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/jake/project/split/val/sigh/f1961_0_sigh.wav'\n",
      "b'/home/jake/project/split/val/sniff/f0605_0_sniff.wav'\n",
      "b'/home/jake/project/split/val/sniff/m3289_0_sniff.wav'\n",
      "b'/home/jake/project/split/val/sigh/m0846_0_sigh.wav'\n",
      "b'/home/jake/project/split/val/cough/f0835_0_cough.wav'\n"
     ]
    }
   ],
   "source": [
    "for f in val_ds.take(5):\n",
    "  print(f.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "598d241e-d97c-497b-84af-1b45a267a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = pathlib.Path('/home/jake/project/split/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "576f00ad-4c66-4221-afb2-b49ce03db5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cough' 'laughter' 'sigh' 'sneeze' 'sniff' 'throatclearing']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted([item.name for item in train_path.glob('*') if item.name != \"LICENSE.txt\"]))\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02d1749f-a470-4b60-8e81-f48868501f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89468985-b9a2-4493-958f-8ff8a856a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n",
      "(None, None)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "397b2b59-ec05-4b91-bf69-128cd4bff100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15531\n",
      "Test: 3591\n",
      "Validation: 1855\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {len(train_ds)}')\n",
    "print(f'Test: {len(test_ds)}')\n",
    "print(f'Validation: {len(val_ds)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66f50067-5df6-4052-b5d4-d62a9d9551d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (38, 129, 1)\n",
      "Label:  4\n",
      "Image shape:  (38, 129, 1)\n",
      "Label:  0\n",
      "Image shape:  (38, 129, 1)\n",
      "Label:  0\n",
      "Image shape:  (38, 129, 1)\n",
      "Label:  3\n",
      "Image shape:  (38, 129, 1)\n",
      "Label:  5\n"
     ]
    }
   ],
   "source": [
    "for image, label in test_ds.take(5):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5556e4-21d3-4f9e-9a0e-7658d8d6ffce",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a70e2f64-a5b7-4547-9896-134ba6a754b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c77ae205-1b53-4b23-a073-2744b741e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (38, 129, 1)\n",
      "Label:  0\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6be46c7-5ba6-4b74-a957-ad82b1950388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=1000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf97348-93d4-48d5-8f07-559655444061",
   "metadata": {},
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d777c4c-9355-4522-938f-c1583362d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 01:22:32.636993: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "  label = label_batch[i]\n",
    "  plt.title(class_names[label])\n",
    "  plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b32ca22c-d0d8-48d9-8003-0e9b213e8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 124, 129, 1)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "print(image_batch.shape)\n",
    "print(label_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c5de7-1b57-423e-a27e-4b3ec1d22117",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5af646c4-5c42-4042-b15a-cd3a557015c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb35e61c-4fec-4ea0-a80a-3dcfee2cfe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.8790045e-09 0.035768136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 01:01:12.796369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c171496a-e641-4d63-833e-8f2bc718a5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([124, 129, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "741ab517-86c5-4905-9e15-0bc18d91c8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7067590ca0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.numpy().astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "009b8404-008d-46c1-875f-5f2297378f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84387f05-90f8-4f1c-b9fb-fa37508d36c6",
   "metadata": {},
   "source": [
    "num_classes = 6\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  #tf.keras.Input(shape=(None,129,1)),\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 1, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 1, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 1, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0e081-a770-4a5f-8ab8-c9406a3f074c",
   "metadata": {},
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004b9c3-c978-4aa5-9384-529cce4bc082",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d0d249e-4feb-48b8-aad2-5cb90c43d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32,(3,3), activation='relu', input_shape=(124,129,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eee6f921-4296-4838-bb80-c9df9b07eb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 122, 127, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 61, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 59, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 29, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 28, 64)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3096640   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,152,774\n",
      "Trainable params: 3,152,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5f5c1ff-be39-4f6f-b318-8798e830452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(6,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8adf3adb-eee5-41c0-8500-fcf605389ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "734e2068-19f1-492e-964f-b8a7f4e6168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.9943 - accuracy: 0.5984 - val_loss: 1.5177 - val_accuracy: 0.4863\n",
      "Epoch 2/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.8789 - accuracy: 0.6480 - val_loss: 1.7635 - val_accuracy: 0.4889\n",
      "Epoch 3/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.7888 - accuracy: 0.6852 - val_loss: 1.9973 - val_accuracy: 0.4819\n",
      "Epoch 4/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.7015 - accuracy: 0.7187 - val_loss: 2.2374 - val_accuracy: 0.4685\n",
      "Epoch 5/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.6411 - accuracy: 0.7521 - val_loss: 2.6383 - val_accuracy: 0.4598\n",
      "Epoch 6/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.5894 - accuracy: 0.7713 - val_loss: 2.7222 - val_accuracy: 0.4744\n",
      "Epoch 7/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.5407 - accuracy: 0.7923 - val_loss: 3.2466 - val_accuracy: 0.4798\n",
      "Epoch 8/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.4911 - accuracy: 0.8106 - val_loss: 3.6149 - val_accuracy: 0.4582\n",
      "Epoch 9/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.4463 - accuracy: 0.8255 - val_loss: 3.8747 - val_accuracy: 0.4668\n",
      "Epoch 10/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.4188 - accuracy: 0.8403 - val_loss: 3.7866 - val_accuracy: 0.4642\n",
      "Epoch 11/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.3800 - accuracy: 0.8553 - val_loss: 4.3678 - val_accuracy: 0.4695\n",
      "Epoch 12/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.3702 - accuracy: 0.8620 - val_loss: 4.1927 - val_accuracy: 0.4695\n",
      "Epoch 13/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.3444 - accuracy: 0.8735 - val_loss: 4.9190 - val_accuracy: 0.4604\n",
      "Epoch 14/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.3317 - accuracy: 0.8792 - val_loss: 5.5814 - val_accuracy: 0.4550\n",
      "Epoch 15/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.3319 - accuracy: 0.8853 - val_loss: 5.3340 - val_accuracy: 0.4534\n",
      "Epoch 16/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2762 - accuracy: 0.8999 - val_loss: 5.4569 - val_accuracy: 0.4577\n",
      "Epoch 17/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2633 - accuracy: 0.9041 - val_loss: 5.2090 - val_accuracy: 0.4712\n",
      "Epoch 18/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2447 - accuracy: 0.9108 - val_loss: 6.6795 - val_accuracy: 0.4593\n",
      "Epoch 19/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2615 - accuracy: 0.9111 - val_loss: 6.3425 - val_accuracy: 0.4539\n",
      "Epoch 20/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2258 - accuracy: 0.9189 - val_loss: 6.7011 - val_accuracy: 0.4507\n",
      "Epoch 21/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2428 - accuracy: 0.9175 - val_loss: 7.1515 - val_accuracy: 0.4518\n",
      "Epoch 22/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2241 - accuracy: 0.9220 - val_loss: 7.4511 - val_accuracy: 0.4544\n",
      "Epoch 23/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2181 - accuracy: 0.9253 - val_loss: 7.0944 - val_accuracy: 0.4598\n",
      "Epoch 24/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1762 - accuracy: 0.9343 - val_loss: 7.8008 - val_accuracy: 0.4356\n",
      "Epoch 25/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1991 - accuracy: 0.9328 - val_loss: 7.0386 - val_accuracy: 0.4485\n",
      "Epoch 26/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1764 - accuracy: 0.9372 - val_loss: 7.9801 - val_accuracy: 0.4485\n",
      "Epoch 27/50\n",
      "486/486 [==============================] - 3s 5ms/step - loss: 0.1654 - accuracy: 0.9390 - val_loss: 8.6451 - val_accuracy: 0.4388\n",
      "Epoch 28/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.2089 - accuracy: 0.9343 - val_loss: 7.6696 - val_accuracy: 0.4550\n",
      "Epoch 29/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1871 - accuracy: 0.9395 - val_loss: 8.4717 - val_accuracy: 0.4544\n",
      "Epoch 30/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1632 - accuracy: 0.9437 - val_loss: 7.8676 - val_accuracy: 0.4550\n",
      "Epoch 31/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1777 - accuracy: 0.9425 - val_loss: 6.6934 - val_accuracy: 0.4496\n",
      "Epoch 32/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1505 - accuracy: 0.9489 - val_loss: 8.3884 - val_accuracy: 0.4561\n",
      "Epoch 33/50\n",
      "486/486 [==============================] - 3s 5ms/step - loss: 0.1345 - accuracy: 0.9506 - val_loss: 8.9174 - val_accuracy: 0.4582\n",
      "Epoch 34/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1521 - accuracy: 0.9491 - val_loss: 7.9654 - val_accuracy: 0.4464\n",
      "Epoch 35/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1710 - accuracy: 0.9474 - val_loss: 9.3124 - val_accuracy: 0.4356\n",
      "Epoch 36/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1514 - accuracy: 0.9505 - val_loss: 9.6357 - val_accuracy: 0.4361\n",
      "Epoch 37/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1409 - accuracy: 0.9528 - val_loss: 9.5053 - val_accuracy: 0.4571\n",
      "Epoch 38/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1265 - accuracy: 0.9549 - val_loss: 9.1659 - val_accuracy: 0.4544\n",
      "Epoch 39/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1183 - accuracy: 0.9578 - val_loss: 9.2858 - val_accuracy: 0.4453\n",
      "Epoch 40/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1212 - accuracy: 0.9590 - val_loss: 9.9224 - val_accuracy: 0.4485\n",
      "Epoch 41/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1238 - accuracy: 0.9577 - val_loss: 10.4631 - val_accuracy: 0.4501\n",
      "Epoch 42/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1229 - accuracy: 0.9584 - val_loss: 9.5611 - val_accuracy: 0.4512\n",
      "Epoch 43/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1367 - accuracy: 0.9575 - val_loss: 9.1945 - val_accuracy: 0.4447\n",
      "Epoch 44/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1171 - accuracy: 0.9619 - val_loss: 10.9651 - val_accuracy: 0.4480\n",
      "Epoch 45/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.0954 - accuracy: 0.9651 - val_loss: 9.9543 - val_accuracy: 0.4512\n",
      "Epoch 46/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1273 - accuracy: 0.9598 - val_loss: 11.3474 - val_accuracy: 0.4415\n",
      "Epoch 47/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1605 - accuracy: 0.9547 - val_loss: 9.9662 - val_accuracy: 0.4383\n",
      "Epoch 48/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1189 - accuracy: 0.9604 - val_loss: 12.4439 - val_accuracy: 0.4544\n",
      "Epoch 49/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1203 - accuracy: 0.9624 - val_loss: 11.2382 - val_accuracy: 0.4404\n",
      "Epoch 50/50\n",
      "486/486 [==============================] - 2s 5ms/step - loss: 0.1150 - accuracy: 0.9645 - val_loss: 10.8700 - val_accuracy: 0.4437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f724870de20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3fd4a-c825-42c4-b0e9-cd81574549ae",
   "metadata": {},
   "source": [
    "### Validate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b7e7b-56d5-4616-9547-41adaa3763c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_latest",
   "language": "python",
   "name": "tf_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
